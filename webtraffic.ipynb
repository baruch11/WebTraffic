{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# load kaggle environment\n",
    "from google.colab import files\n",
    "files.upload() #upload kaggle.json\n",
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!mkdir logs\n",
    "\n",
    "!kaggle competitions download -c web-traffic-time-series-forecasting\n",
    "!unzip train_2.csv.zip\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1111)\n",
    "p = 0.2  # 1% of the lines\n",
    "# keep the header, then take only 1% of lines\n",
    "# if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
    "df = pd.read_csv(\n",
    "         \"train_2.csv.zip\",\n",
    "         header=0, \n",
    "         skiprows=lambda i: i>0 and random.random() > p\n",
    ")\n",
    "df = df.sample(frac=1).reset_index(drop=True).fillna(0)\n",
    "Page = df.Page\n",
    "agent = Page.apply(lambda x: x.split(\"_\")[-1])\n",
    "access = Page.apply(lambda x: x.split(\"_\")[-2])\n",
    "wikiproject = Page.apply(lambda x: x.split(\"_\")[-3])\n",
    "page_name = Page.apply(lambda x: \"_\".join(x.split(\"_\")[:-3]))\n",
    "df.drop(columns=[\"Page\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mtraffic = df.values\n",
    "#Mtraffic = Mtraffic/Mtraffic.max()\n",
    "\n",
    "output_len = 62\n",
    "\n",
    "Ltst = 1000\n",
    "Ltr = Mtraffic.shape[0] - 2*Ltst\n",
    "shift_start = 0\n",
    "\n",
    "x_train, y_train = Mtraffic[:Ltr,shift_start:-output_len], Mtraffic[:Ltr,-output_len:]\n",
    "x_valid, y_valid = Mtraffic[Ltr:Ltr+Ltst, shift_start:-output_len], Mtraffic[Ltr:Ltr+Ltst,-output_len:]\n",
    "x_test, y_test = Mtraffic[Ltr+Ltst:, shift_start:-output_len], Mtraffic[Ltr+Ltst:,-output_len:]\n",
    "\n",
    "x_train.shape, y_train.shape\n",
    "x_train.shape, x_valid.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return tf.reduce_mean(2 * tf.math.abs(F - A) / (tf.math.abs(A) + tf.math.abs(F) + 1e-16)) * 100 \n",
    "\n",
    "def smape_reg(A, F):\n",
    "    epsilon = 0.1\n",
    "    summ = tf.maximum(tf.abs(A) + tf.abs(F) + epsilon, 0.5 + epsilon)\n",
    "    return tf.abs(A - F) / summ * 2.0 * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_np(A, F):\n",
    "    return 100/A.size * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F) + np.finfo(float).eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard callbacks\n",
    "from datetime import datetime\n",
    "datetime.now().strftime(\"%H-%M-%S\")\n",
    "def create_tb_cb(model_name):\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\"+model_name+\"-\"+datetime.now().strftime(\"%H-%M-%S\"),\n",
    "                                          histogram_freq=10\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_check_result(df_check, predict_func, ax):\n",
    "    Ntsteps = x_train.shape[1]\n",
    "    pred = predict_func(df_check.values[:,:-62])\n",
    "    print(\"smape \", smape_np(pred, df_check.values[:,-62:]))\n",
    "    df_ref = df_check.reset_index(drop=True).T.reset_index(drop=True)\n",
    "    df_pred = pd.DataFrame(pred.T,index=np.arange(0,62)+Ntsteps)\n",
    "    df_pred.plot(ax=ax, style=\".-\")\n",
    "    #ax.set_prop_cycle(None)\n",
    "    df_ref.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[linear model](#linear-model)  \n",
    "[RNN](#RNN)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(agent)\n",
    "ax.plot(access)\n",
    "ax.plot(wikiproject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "line = ax.plot(df.loc[agent!=\"spider\"].head().T.reset_index(drop=True))\n",
    "ax.legend(line, Page.head())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_repeated_estimator(ts_prev):\n",
    "    \"\"\" ts_prev : matrix (m, Ts) \"\"\"\n",
    "    return np.tile(ts_prev[:,-1].reshape(-1,1), (1,62))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dnum = df.fillna(0).values[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_np(np.zeros_like(Dnum), Dnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mmean = np.tile(np.mean(Dnum, 1).reshape(-1,1) , (1,Dnum.shape[1]))\n",
    "smape_np(Mmean, Dnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_autocorrelation(x):\n",
    "    \"\"\"\n",
    "    http://stackoverflow.com/q/14297012/190597\n",
    "    http://en.wikipedia.org/wiki/Autocorrelation#Estimation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    variance = x.var()\n",
    "    x = x-x.mean()\n",
    "    r = np.correlate(x, x, mode = 'full')[-n:]\n",
    "    assert np.allclose(r, np.array([(x[:n-k]*x[-(n-k):]).sum() for k in range(n)]))\n",
    "    result = r/(variance*(np.arange(n, 0, -1)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allagent = df.loc[agent==\"all-agents\"]\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(estimated_autocorrelation(df_allagent.iloc[20951,:].values))\n",
    "#ax.plot(estimated_autocorrelation(df_allagent.iloc[1,:].values))\n",
    "#ax.plot(estimated_autocorrelation(df_allagent.iloc[2,:].values))\n",
    "#ax.plot(estimated_autocorrelation(df_allagent.iloc[3,:].values))\n",
    "ax.scatter(365,0,s=10,c=\"r\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(df_allagent.iloc[20951,:].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allagent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allagent.mean(axis=1).sort_values(ascending=False).head(15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repeat last value](#repeat-last-value)  \n",
    "[linear model](#linear-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### repeat last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_lv(X):\n",
    "    return np.tile(X[:,-1].reshape(-1,1), reps=(1,output_len))\n",
    "\n",
    "smape_np(repeat_lv(x_train), y_train), smape_np(repeat_lv(x_valid), y_valid), smape_np(repeat_lv(x_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_lv(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tb_cb = create_tb_cb(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=output_len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear.compile(loss=smape_reg, optimizer=tf.optimizers.Adam(learning_rate=1e-4), \n",
    "                     metrics=[smape,\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 40:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.05)\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntr = 50000\n",
    "tmp_x = x_train[:Ntr,:]; tmp_y = y_train[:Ntr,:]\n",
    "model_linear.fit(tmp_x, tmp_y, epochs=100, batch_size=32, \n",
    "                 validation_data= (x_valid, y_valid), \n",
    "                 callbacks=[tb_cb, lr_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_estimator(x):\n",
    "    return model_linear.predict(x[:,-x_train.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_linear.get_layer(\"dense\").get_weights()[0]\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "ax.plot(np.abs(weights[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_np(linear_estimator(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots()\n",
    "\n",
    "plot_check_result(df.iloc[[50],:], linear_estimator, ax)\n",
    "#ax.set_xlim(left=700)\n",
    "#ax.set_ylim((0,300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -Rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Limit_train=5000\n",
    "MaxTs = 200\n",
    "Nneurons = 20\n",
    "Nlayers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simn = 'Ltr'+str(Limit_train)+'-Ts'+str(MaxTs)+'-Nn'+str(Nneurons)+'-Nl'+str(Nlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tb_cb = create_tb_cb(simn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(tf.keras.Input((MaxTs,1)))\n",
    "if Nlayers==0:\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "for ii in range(Nlayers-1):\n",
    "    model.add(tf.keras.layers.GRU(Nneurons, return_sequences=True))\n",
    "if Nlayers>0:\n",
    "    model.add(tf.keras.layers.GRU(Nneurons))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(output_len))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_train(x):\n",
    "    return np.max(x, axis=1).reshape(-1,1)+1e-10\n",
    "    return np.ones((x.shape[0],1))\n",
    "    scaler = MinMaxScaler((0,1))\n",
    "    scaler.fit(x[:,-MaxTs:].T)\n",
    "    return scaler.transform(x.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((x_train.shape[0],1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rnn = (x_train/scale_train(x_train))[:Limit_train,-MaxTs:, np.newaxis]; \n",
    "y_train_rnn = (y_train/scale_train(x_train))[:Limit_train]\n",
    "x_valid_rnn = (x_valid/scale_train(x_valid))[:Limit_train,-MaxTs:, np.newaxis]; \n",
    "y_valid_rnn = (y_valid/scale_train(x_valid))[:Limit_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%matplotlib inline\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(x_train_rnn[0,:,0])\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=smape, optimizer=tf.optimizers.Adam(learning_rate=1e-3),metrics=[smape, \"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_rnn, y_train_rnn, epochs=200, batch_size=32 , #validation_split= 0.2,\n",
    "          validation_data= (x_valid_rnn, y_valid_rnn), \n",
    "          callbacks=[tb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(x):\n",
    "    xsc = x/(np.max(x,axis=1).reshape(-1,1)+1e-10)\n",
    "    pred0 = model.predict(xsc[:, -MaxTs:, np.newaxis])\n",
    "    #return pred0    \n",
    "    return pred0 * np.max(x,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = rnn_predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_np(pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "f,ax = plt.subplots()\n",
    "\n",
    "plot_check_result(df.iloc[[50],:], rnn_predict, ax)\n",
    "#ax.set_xlim(left=700)\n",
    "#ax.set_ylim(top=1000,bottom=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = pd.read_csv(\"key_2.csv.zip\").set_index(\"Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key[\"Visits\"] = None\n",
    "key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_form(_df, _estimator=None):\n",
    "    \"\"\" return a serie indexed by Page \"\"\"\n",
    "    out_date = pd.date_range(start=\"2017-09-13\", end=\"2017-11-13\", freq=\"1D\").strftime(\"%Y-%m-%d\").to_list()\n",
    "    num_hist = _df.drop(columns=\"Page\").fillna(0).values\n",
    "    num_pred = _estimator(num_hist)\n",
    "    ret = pd.DataFrame(num_pred, columns=out_date, index=_df[\"Page\"]).stack().rename(\"Visits\")\n",
    "    ret.index = [ii[0]+\"_\"+ii[1] for ii in ret.index]\n",
    "    return ret\n",
    "\n",
    "chunk = pd.read_csv(\"train_2.csv.zip\", nrows=10000)\n",
    "Visits_pred = output_form(chunk, linear_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = pd.read_csv(\"train_2.csv.zip\", chunksize=10000)\n",
    "\n",
    "for ii, chunk in enumerate(df_chunk):\n",
    "    print(\"Prediction {}\".format(ii))\n",
    "    predictions = output_form(chunk, rnn_predict).astype(int)\n",
    "    key.loc[predictions.index, \"Visits\"] = predictions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key.to_csv(\"subm_gru.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
