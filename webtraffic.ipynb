{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "%config Completer.use_jedi = False  # to make autocompletion faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# load kaggle environment if in google colab\n",
    "from google.colab import files\n",
    "files.upload() #upload kaggle.json\n",
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!mkdir logs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!kaggle competitions download -c web-traffic-time-series-forecasting\n",
    "!yes|unzip web-traffic-time-series-forecasting.zip\n",
    "!yes|unzip train_2.csv.zip\n",
    "!tail -n +2 train_2.csv|shuf --random-source train_2.csv > train_2_shuffled.csv\n",
    "!head -n -10000 train_2_shuffled.csv > train_set.csv\n",
    "!tail -n 10000 train_2_shuffled.csv > validation_set.csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_len = 62\n",
    "normalize_ds = False\n",
    "batch_size = 4096\n",
    "\n",
    "def process_line(line, normalize_ds=False):\n",
    "    line = tf.io.decode_csv(line, record_defaults=[\"\"]+[0.]*803)\n",
    "    # categorical features \n",
    "    agent = tf.strings.split(line[0], sep=\"_\")[-1]\n",
    "    access = tf.strings.split(line[0], sep=\"_\")[-2]\n",
    "    project = tf.strings.split(line[0], sep=\"_\")[-3]\n",
    "    traffic = tf.stack(line[1:])\n",
    "    page = tf.strings.split(line[0], sep=\"_\")[-4]\n",
    "\n",
    "    if normalize_ds:\n",
    "        traffic = traffic / (tf.reduce_max(traffic) +1e-10)\n",
    "    return (agent, access, project, traffic[:-output_len], page), traffic[-output_len:]\n",
    "    #return tf.stack(line[1:-62]), tf.stack(line[-62:])\n",
    "\n",
    "    \n",
    "def make_dataset(ds0,nmax=None, normalize_ds=False , batch_size = 32):\n",
    "    ds1 = ds0.map(lambda x: process_line(x, normalize_ds))\n",
    "    if nmax is not None:\n",
    "        ds1 = ds1.take(nmax)\n",
    "    return ds1.batch(batch_size).prefetch(1).cache()\n",
    "\n",
    "ds = make_dataset(tf.data.TextLineDataset(\"train_set.csv\"), normalize_ds=normalize_ds, batch_size=batch_size)\n",
    "val_ds = make_dataset(tf.data.TextLineDataset(\"validation_set.csv\"), normalize_ds=normalize_ds , batch_size=batch_size)\n",
    "ds_short = make_dataset(tf.data.TextLineDataset(\"train_set.csv\"), normalize_ds=normalize_ds,nmax=50000, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cat = tf.data.TextLineDataset(\"train_set.csv\").map(lambda x: process_line(x, normalize_ds)).batch(50000).take(1)\n",
    "np_cat = list(ds_cat.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webtraffic_utils import OneHotEncodingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotAgent = OneHotEncodingLayer()\n",
    "onehotAgent.adapt(list(np.unique(np_cat[0][0][0])))\n",
    "onehotAccess = OneHotEncodingLayer()\n",
    "onehotAccess.adapt(list(np.unique(np_cat[0][0][1])))\n",
    "onehotProject = OneHotEncodingLayer()\n",
    "onehotProject.adapt(list(np.unique(np_cat[0][0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotProject.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repeat last value](#repeat-last-value)  \n",
    "[linear model](#linear-model)  \n",
    "[RNN](#RNN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return tf.reduce_mean(2 * tf.math.abs(F - A) / (tf.math.abs(A) + tf.math.abs(F) + 1e-16)) * 100 \n",
    "\n",
    "def smape_reg(A, F):\n",
    "    epsilon = 1e-3\n",
    "    summ = tf.maximum(tf.abs(A) + tf.abs(F) + epsilon, 0.5 + epsilon)\n",
    "    return tf.abs(A - F) / summ * 2.0 * 100\n",
    "\n",
    "def smape_np(A, F):\n",
    "    return 100/A.size * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F) + np.finfo(float).eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard callbacks\n",
    "from datetime import datetime\n",
    "\n",
    "def create_tb_cb(model_name):\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=\"./logs/\"+model_name+\"-\"+datetime.now().strftime(\"%H-%M-%S\"),\n",
    "                                          histogram_freq=10\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_check_result(x_check, predict_func, ax):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x_check: np.array\n",
    "    \"\"\"\n",
    "    pred = predict_func(x_check[:-output_len])\n",
    "    ax.plot(x_check)\n",
    "    ax.plot(np.arange(output_len)+len(x_check)-output_len, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_autocorrelation(x):\n",
    "    \"\"\"\n",
    "    http://stackoverflow.com/q/14297012/190597\n",
    "    http://en.wikipedia.org/wiki/Autocorrelation#Estimation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    variance = x.var()\n",
    "    x = x-x.mean()\n",
    "    r = np.correlate(x, x, mode = 'full')[-n:]\n",
    "    assert np.allclose(r, np.array([(x[:n-k]*x[-(n-k):]).sum() for k in range(n)]))\n",
    "    result = r/(variance*(np.arange(n, 0, -1)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrest(x, ax):\n",
    "    fft = tf.signal.rfft(x-np.mean(x))\n",
    "    T = len(fft)\n",
    "    ax.plot(np.abs(fft))\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.grid()\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([2*T/7., 2*T/30.5, 2*T/365.])\n",
    "    ax.set_xticklabels([\"weekly\", \"monthly\", \"yearly\"], rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify some interessant line\n",
    "df_examples = pd.read_csv(\"train_2.csv.zip\", header=0, nrows=50000).set_index(\"Page\")\n",
    "agent = pd.Series(df_examples.index.map(lambda x: x.split(\"_\")[-1]), df_examples.index)\n",
    "access = pd.Series(df_examples.index.map(lambda x: x.split(\"_\")[-2]), df_examples.index)\n",
    "project = pd.Series(df_examples.index.map(lambda x: x.split(\"_\")[-3]), df_examples.index)\n",
    "df_allagent = df_examples.loc[agent==\"all-agents\"]\n",
    "iter_allagent = df_allagent.iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.nunique(),access.nunique(),project.nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f, ax= plt.subplots()\n",
    "page,row = next(iter_allagent)\n",
    "plot_spectrest(row.values, ax)\n",
    "ax.set_title(page)\n",
    "row.fillna(0, inplace=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f,(ax1,ax) = plt.subplots(2,1, sharex=True)\n",
    "\n",
    "x = df.loc[page].values\n",
    "ax.plot(estimated_autocorrelation(x))\n",
    "ax.grid()\n",
    "ax1.plot(np.log1p(x))\n",
    "ax1.grid()\n",
    "plt.show()\n",
    "#estimated_autocorrelation(df.loc[\"France_fr.wikipedia.org_desktop_all-agents\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remarkable pages\n",
    "rem_pages = [\n",
    "    'Acier_inoxydable_fr.wikipedia.org_desktop_all-agents',\n",
    "]\n",
    "page = rem_pages[0]\n",
    "traffic_t =  df_examples.loc[page].values\n",
    "f,vax = plt.subplots(1,3, figsize=(20,4))\n",
    "fax = vax.flat\n",
    "\n",
    "\n",
    "ax=next(fax)\n",
    "ax.plot(traffic_t)\n",
    "ax.set_title(\"time traffic\")\n",
    "\n",
    "\n",
    "ax=next(fax)\n",
    "ax.plot(estimated_autocorrelation(traffic_t))\n",
    "ax.set_xticks([0,365,2*365])\n",
    "ax.set_title(\"autocorrelation\")\n",
    "ax.grid()\n",
    "\n",
    "ax=next(fax)\n",
    "plot_spectrest(traffic_t, ax)\n",
    "ax.set_title(\"spectral estimation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle(page, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repeat last value](#repeat-last-value)  \n",
    "[linear model](#linear-model)  \n",
    "[RNN](#RNN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat last value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatLastValue(tf.keras.Model):\n",
    "    def call(self, inputs):\n",
    "        #print(inputs)\n",
    "        Xtraff = inputs[3]\n",
    "        return tf.tile(Xtraff[:,-2:-1], tf.constant([1,output_len], tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlv = RepeatLastValue()\n",
    "rlv.compile(loss=smape_reg, metrics=[smape,\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlv.evaluate(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlv_estimator(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x np.array, len=741\n",
    "    \"\"\"\n",
    "    fake_feature = np.array([b\"\"], dtype=object)\n",
    "    fnorm = 1.0\n",
    "    if normalize_ds:\n",
    "        fnorm = np.max(x)\n",
    "    return rlv.predict((fake_feature, fake_feature, fake_feature, x.reshape(1,-1)/fnorm))[0] * fnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "plot_check_result(df_examples.loc[page].values, rlv_estimator, ax)\n",
    "ax.set_title(page)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tb_cb = create_tb_cb(\"linear\")\n",
    "\n",
    "class preprocessing(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs[:,-150:]\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(741,))\n",
    "agent = tf.keras.layers.Input(shape=())\n",
    "access = tf.keras.layers.Input(shape=())\n",
    "project = tf.keras.layers.Input(shape=())\n",
    "page = tf.keras.layers.Input(shape=())\n",
    "x = preprocessing()(inputs)\n",
    "outputs = tf.keras.layers.Dense(units=output_len)(x)\n",
    "\n",
    "model_linear = tf.keras.Model(inputs=[agent, access, project, inputs, page], outputs=[outputs])\n",
    "\n",
    "model_linear.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear.compile(loss=smape_reg, optimizer=tf.optimizers.Adam(learning_rate=1e-4), \n",
    "                     metrics=[smape,\"mae\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.05)\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        #print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "        weights = self.model.get_layer(\"dense\").get_weights()[0]\n",
    "        print(\" weights mean \", np.mean(weights), \" max \", np.max(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.cache()\n",
    "model_linear.fit(ds, epochs=100, callbacks=[tb_cb, lr_cb], validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_linear.get_layer(\"dense\").get_weights()[0]\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "ax.plot(np.abs(weights[:,0]))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_estimator(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x np.array, len=741\n",
    "    \"\"\"\n",
    "    fake_feature = np.array([b\"\"], dtype=object)\n",
    "    return model_linear.predict((fake_feature, fake_feature, fake_feature,[x.reshape(1,-1)]))[0]\n",
    "\n",
    "#linear_estimator(df_examples.loc[page].values[:-output_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "plot_check_result(df_examples.loc[page].values, linear_estimator, ax)\n",
    "ax.set_title(page)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -Rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalize_rnn(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        fact = tf.reduce_max(inputs, axis=1, keepdims=True)\n",
    "        ret = tf.divide(inputs, fact + 1e-10) \n",
    "        return ret, fact\n",
    "\n",
    "\n",
    "class denormalize_rnn(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, fact):\n",
    "        #ret = tf.maximum(tf.floor(tf.multiply(inputs, fact)), 0)\n",
    "        ret = tf.multiply(inputs, fact)\n",
    "        return ret\n",
    "\n",
    "#xtry = tf.constant(np.random.randint(-1000, 1000, size=(10,5)), dtype=tf.float32)\n",
    "#xn, fact = normalize_rnn()(xtry)\n",
    "#denormalize_rnn()(xn,fact),xtry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nneurons = 20\n",
    "Nlayers = 2\n",
    "MaxTs = 200\n",
    "usePastYear = True\n",
    "useMetadata = True\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "simn = 'Ts'+str(MaxTs)+'-Nn'+str(Nneurons)+'-Nl'+str(Nlayers)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tb_cb = create_tb_cb(simn)\n",
    "\n",
    "class preprocessing_rnn(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, agent1h, access1h):\n",
    "        ret = inputs[:,-MaxTs:,np.newaxis]\n",
    "        if useMetadata:\n",
    "            agent_broadcast = tf.tile(agent1h[:,np.newaxis,:],[1,MaxTs,1])\n",
    "            ret = tf.concat([ret, agent_broadcast], axis=2)\n",
    "            access_broadcast = tf.tile(access1h[:,np.newaxis,:],[1,MaxTs,1])\n",
    "            ret = tf.concat([ret, access_broadcast], axis=2)\n",
    "            \n",
    "        if usePastYear:\n",
    "            pastYear = inputs[:, -MaxTs-365+output_len:-365+output_len, np.newaxis]\n",
    "            ret = tf.concat([ret, pastYear], axis=2)\n",
    "        return ret\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(741,))\n",
    "I_agent = tf.keras.layers.Input(shape=(), dtype=object)\n",
    "agent1h = onehotAgent(I_agent)\n",
    "I_access = tf.keras.layers.Input(shape=(), dtype=object)\n",
    "access1h = onehotAccess(I_access)\n",
    "\n",
    "I_project = tf.keras.layers.Input(shape=(), dtype=object)\n",
    "I_page = tf.keras.layers.Input(shape=())\n",
    "\n",
    "\n",
    "x, factors = normalize_rnn()(inputs)\n",
    "x = preprocessing_rnn()(x, agent1h, access1h)\n",
    "for ii in range(Nlayers-1):\n",
    "    x = tf.keras.layers.GRU(Nneurons, return_sequences=True)(x)\n",
    "x = tf.keras.layers.GRU(Nneurons)(x)\n",
    "x= tf.keras.layers.Dense(output_len)(x)\n",
    "outputs= denormalize_rnn()(x, factors)\n",
    "\n",
    "model_rnn = tf.keras.Model(inputs=[I_agent, I_access, I_project, inputs, I_page], outputs=[outputs])\n",
    "\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss=smape_reg, optimizer=tf.optimizers.Adam(learning_rate=1e-4),metrics=[smape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.fit(ds_short, epochs=100, callbacks=[tb_cb], validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_estimator(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x np.array, len=741\n",
    "    \"\"\"\n",
    "    fake_feature = np.array([b\"\"], dtype=object)\n",
    "    fnorm = 1.0\n",
    "    if normalize_ds:\n",
    "        fnorm = np.max(x)\n",
    "    return model_rnn.predict((fake_feature, fake_feature, fake_feature,[x.reshape(1,-1)/fnorm]))[0] * fnorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, ax = plt.subplots()\n",
    "plot_check_result(df_examples.loc[page].values, rnn_estimator, ax)\n",
    "ax.set_title(page)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_pred(ds_short):\n",
    "    pred = model_rnn.predict(ds_short)\n",
    "    lds = list(ds_short.as_numpy_iterator())\n",
    "    agent = np.concatenate([batch[0][0] for batch in lds])\n",
    "    access = np.concatenate([batch[0][1] for batch in lds])\n",
    "    ytrue = np.concatenate([batch[1] for batch in lds])\n",
    "    xtrain = np.concatenate([batch[0][3] for batch in lds])\n",
    "    return pred, xtrain, ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_row(A, F):\n",
    "    return np.mean(100 * (2 * np.abs(F - A) / (np.abs(A) + np.abs(F) + np.finfo(float).eps)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, x_train, y_train = get_ds_pred(ds_short)\n",
    "pred_val, x_val, y_val = get_ds_pred(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax =plt.subplots()\n",
    "kwargs = {\"alpha\": 0.5, \"bins\": 50}\n",
    "ax.hist(smape_row(pred_train, y_train),**kwargs, label=\"train\")\n",
    "ax.hist(smape_row(pred_val, y_val),**kwargs, label=\"validation\")\n",
    "ax.set_title(np.mean(smape_row(pred, ytrue)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsm = smape_row(pred_train, y_train)\n",
    "np.mean(xsm[xsm < 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsmape = pd.Series(smape_row(pred_train, y_train))\n",
    "vsmape[(vsmape>150) ].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "ii= 49\n",
    "f, ax = plt.subplots()\n",
    "plot_check_result(np.r_[x_train[ii], y_train[ii]], rnn_estimator, ax)\n",
    "#ax.set_title(page)\n",
    "#ax.set_yscale(\"log\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train[ii].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = pd.read_csv(\"key_2.csv.zip\").set_index(\"Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key[\"Visits\"] = None\n",
    "key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_form(_df, _estimator=None):\n",
    "    \"\"\" return a serie indexed by Page \"\"\"\n",
    "    out_date = pd.date_range(start=\"2017-09-13\", end=\"2017-11-13\", freq=\"1D\").strftime(\"%Y-%m-%d\").to_list()\n",
    "    num_hist = _df.drop(columns=\"Page\").fillna(0).values\n",
    "    num_pred = _estimator(num_hist)\n",
    "    ret = pd.DataFrame(num_pred, columns=out_date, index=_df[\"Page\"]).stack().rename(\"Visits\")\n",
    "    ret.index = [ii[0]+\"_\"+ii[1] for ii in ret.index]\n",
    "    return ret\n",
    "\n",
    "chunk = pd.read_csv(\"train_2.csv.zip\", nrows=10000)\n",
    "Visits_pred = output_form(chunk, linear_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = pd.read_csv(\"train_2.csv.zip\", chunksize=10000)\n",
    "\n",
    "for ii, chunk in enumerate(df_chunk):\n",
    "    print(\"Prediction {}\".format(ii))\n",
    "    predictions = output_form(chunk, rnn_predict).astype(int)\n",
    "    key.loc[predictions.index, \"Visits\"] = predictions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key.to_csv(\"subm_gru.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
